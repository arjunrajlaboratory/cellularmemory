# Normalize cy3, a594, and cy5 by cy7
all_data <- all_data %>%
dplyr::mutate(cy3_norm = cy3 / cy7,
a594_norm = a594 / cy7,
cy5_norm = cy5 / cy7)
#normalize by area
all_data <- all_data %>%
dplyr::mutate(micron2 = `cell.Blob.Blob.metrics...Area` * 0.0467) %>%
dplyr::mutate(cy3_areanorm_RNApermicron2 = cy3 / `micron2`,
a594_areanorm_RNApermicron2 = a594 / `micron2`,
cy5_areanorm_RNApermicron2 = cy5 / `micron2`)
# Calculate the mean for each area-normalized variable, sample, and dataset
mean_areanorm_data <- all_data %>%
group_by(dataset_identifier, sample_identifier) %>%
dplyr::summarise(
cy3_mean = mean(cy3_areanorm_RNApermicron2, na.rm = TRUE),
cy3_se = sd(cy3_areanorm_RNApermicron2, na.rm = TRUE) / sqrt(n()),
a594_mean = mean(a594_areanorm_RNApermicron2, na.rm = TRUE),
a594_se = sd(a594_areanorm_RNApermicron2, na.rm = TRUE) / sqrt(n()),
cy5_mean = mean(cy5_areanorm_RNApermicron2, na.rm = TRUE),
cy5_se = sd(cy5_areanorm_RNApermicron2, na.rm = TRUE) / sqrt(n()),
.groups = "drop"
) %>%
pivot_longer(
cols = contains("_mean") | contains("_se"),
names_to = c(".value", "variable"),
names_sep = "_"
)
# Split the data into means and SEs
mean_data <- mean_areanorm_data %>%
filter(variable == "mean") %>%
dplyr::select(-variable)
se_data <- mean_areanorm_data %>%
filter(variable == "se") %>%
dplyr::select(-variable)
# Join means and SEs
analyte_data <- full_join(mean_data, se_data, by = c("dataset_identifier", "sample_identifier"), suffix = c("_mean", "_se"))
# Pivot longer to have one row per analyte per dataset_identifier per sample_identifier
long_data <- analyte_data %>%
pivot_longer(cols = -c(dataset_identifier, sample_identifier), names_pattern = "(.*)_(mean|se)", names_to = c("analyte", ".value"))
processed_data <- long_data
overall_means <- processed_data %>%
group_by(sample_identifier, analyte) %>%
summarise(overall_mean = mean(mean, na.rm = TRUE), .groups = 'drop')
dodge_width = .7
# Creating the plot with individual experiment bars and error bars
plot <- ggplot() +
# Add individual experiment bars with dodging
geom_bar(data = processed_data, aes(x = sample_identifier, y = mean, fill = dataset_identifier, group = interaction(sample_identifier, dataset_identifier, analyte)),
stat = "identity", position = position_dodge(width = dodge_width), width = 0.6) +
# Add error bars with dodging, aligned with individual bars
geom_errorbar(data = processed_data, aes(x = sample_identifier, ymin = mean - se, ymax = mean + se, group = interaction(sample_identifier, dataset_identifier, analyte)),
position = position_dodge(width = dodge_width), width = 0.25) +
# Overlay overall mean bars as translucent with a wider dodge to cover all individual bars
geom_bar(data = overall_means, aes(x = sample_identifier, y = overall_mean, fill = analyte, group = sample_identifier),
stat = "identity", position = position_dodge(width = dodge_width * 1.2), width = dodge_width * 1.1, alpha = 0.5, color = "black", show.legend = FALSE) +
facet_wrap(~analyte, scales = "free_y") +
theme_minimal() +
labs(title = "Mean Values with SE and Overall Mean for Each Condition", x = "Sample Identifier", y = "Mean Value") +
scale_fill_brewer(palette = "Set3")
print(plot)
inputdirectories <- c("/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/dexmemfish/rep7/all"
)
inputdirectories <- c("/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/dexmemfish/rep7/all"
)
outputdirectory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/dexmemfish"
# Function to safely convert to numeric
convert_to_numeric <- function(df, cols_to_convert) {
existing_cols <- intersect(cols_to_convert, names(df))
df[existing_cols] <- lapply(df[existing_cols], function(x) as.numeric(as.character(x)))
return(df)
}
# Columns to convert
cols_to_convert <- c("cy3", "a594", "cy5", "cy7","cell.Blob.Blob.metrics...Area","cell.Blob.Blob.metrics...Perimeter","cell.Blob.Blob.metrics...Centroid...y","cell.Blob.Blob.metrics...Centroid...x")
# Modified function to extract numeric identifier before the underscore
extract_first_number_after_dash <- function(filename) {
# Extracting the first number after the "-"
sub(".*-([0-9]+).*\\.csv$", "\\1", basename(filename))
}
# Initialize an empty list to store data from all directories
all_data_list <- list()
# Adding a unique dataset identifier for each input directory
dataset_id <- 1
# Iterate over each input directory and append data
for(inputdirectory in inputdirectories) {
files <- list.files(path = inputdirectory, pattern = "*.csv", full.names = TRUE)
data_list <- lapply(files, function(f) {
df <- read.csv(f, stringsAsFactors = FALSE)
df <- convert_to_numeric(df, cols_to_convert)
df$sample_identifier <- extract_first_number_after_dash(f) # Adding the modified identifier
df$dataset_identifier <- paste("Dataset", dataset_id) # Adding the dataset identifier
return(df)
})
all_data_list <- c(all_data_list, data_list) # Append data from current directory
dataset_id <- dataset_id + 1 # Increment dataset identifier for the next directory
}
# Combining all data into one dataframe
all_data <- bind_rows(all_data_list, .id = "file_id")
# Normalize cy3, a594, and cy5 by cy7
all_data <- all_data %>%
dplyr::mutate(cy3_norm = cy3 / cy7,
a594_norm = a594 / cy7,
cy5_norm = cy5 / cy7)
#normalize by area
all_data <- all_data %>%
dplyr::mutate(micron2 = `cell.Blob.Blob.metrics...Area` * 0.0467) %>%
dplyr::mutate(cy3_areanorm_RNApermicron2 = cy3 / `micron2`,
a594_areanorm_RNApermicron2 = a594 / `micron2`,
cy5_areanorm_RNApermicron2 = cy5 / `micron2`)
# Calculate the mean for each area-normalized variable, sample, and dataset
mean_areanorm_data <- all_data %>%
group_by(dataset_identifier, sample_identifier) %>%
dplyr::summarise(
cy3_mean = mean(cy3_areanorm_RNApermicron2, na.rm = TRUE),
cy3_se = sd(cy3_areanorm_RNApermicron2, na.rm = TRUE) / sqrt(n()),
a594_mean = mean(a594_areanorm_RNApermicron2, na.rm = TRUE),
a594_se = sd(a594_areanorm_RNApermicron2, na.rm = TRUE) / sqrt(n()),
cy5_mean = mean(cy5_areanorm_RNApermicron2, na.rm = TRUE),
cy5_se = sd(cy5_areanorm_RNApermicron2, na.rm = TRUE) / sqrt(n()),
.groups = "drop"
) %>%
pivot_longer(
cols = contains("_mean") | contains("_se"),
names_to = c(".value", "variable"),
names_sep = "_"
)
# Split the data into means and SEs
mean_data <- mean_areanorm_data %>%
filter(variable == "mean") %>%
dplyr::select(-variable)
se_data <- mean_areanorm_data %>%
filter(variable == "se") %>%
dplyr::select(-variable)
# Join means and SEs
analyte_data <- full_join(mean_data, se_data, by = c("dataset_identifier", "sample_identifier"), suffix = c("_mean", "_se"))
# Pivot longer to have one row per analyte per dataset_identifier per sample_identifier
long_data <- analyte_data %>%
pivot_longer(cols = -c(dataset_identifier, sample_identifier), names_pattern = "(.*)_(mean|se)", names_to = c("analyte", ".value"))
processed_data <- long_data
overall_means <- processed_data %>%
group_by(sample_identifier, analyte) %>%
summarise(overall_mean = mean(mean, na.rm = TRUE), .groups = 'drop')
dodge_width = .7
# Creating the plot with individual experiment bars and error bars
plot <- ggplot() +
# Add individual experiment bars with dodging
geom_bar(data = processed_data, aes(x = sample_identifier, y = mean, fill = dataset_identifier, group = interaction(sample_identifier, dataset_identifier, analyte)),
stat = "identity", position = position_dodge(width = dodge_width), width = 0.6) +
# Add error bars with dodging, aligned with individual bars
geom_errorbar(data = processed_data, aes(x = sample_identifier, ymin = mean - se, ymax = mean + se, group = interaction(sample_identifier, dataset_identifier, analyte)),
position = position_dodge(width = dodge_width), width = 0.25) +
# Overlay overall mean bars as translucent with a wider dodge to cover all individual bars
geom_bar(data = overall_means, aes(x = sample_identifier, y = overall_mean, fill = analyte, group = sample_identifier),
stat = "identity", position = position_dodge(width = dodge_width * 1.2), width = dodge_width * 1.1, alpha = 0.5, color = "black", show.legend = FALSE) +
facet_wrap(~analyte, scales = "free_y") +
theme_minimal() +
labs(title = "Mean Values with SE and Overall Mean for Each Condition", x = "Sample Identifier", y = "Mean Value") +
scale_fill_brewer(palette = "Set3")
print(plot)
ggsave(filename = paste0(outputdirectory, "/rep7.pdf"), plot = plot, width = 12, height = 8, device = "pdf")
#####
all_data_cleaned <- dplyr::filter(all_data, Shape == "polygon")
# Create the summary table
summary_table <- all_data_cleaned %>%
filter(dataset_identifier %in% paste0("Dataset ", 1:6), sample_identifier %in% as.character(1:6)) %>%
group_by(dataset_identifier, sample_identifier) %>%
summarize(count = n()) %>%
ungroup()
# Pivot the table to get dataset_identifier as rows and file_id as columns
pivot_table <- summary_table %>%
pivot_wider(names_from = sample_identifier, values_from = count, values_fill = list(count = 0))
inputdirectory <- "/Users/jess/Dropbox (RajLab)/Shared_JessL/paper/extractedData/dexmemfish/rep7"
inputdirectory <- "/Users/jess/Dropbox (RajLab)/Shared_JessL/paper/extractedData/dexmemfish/rep7/all"
outputdirectory <- "/Users/jess/Dropbox (RajLab)/Shared_JessL/paper/plots/dexmemfish/rep7"
inputdirectory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/dexmemfish/rep7/all"
outputdirectory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/dexmemfish/rep7"
source("/Users/jess/Dropbox (RajLab)/Shared_JessL/paper/plotScripts/dexmemfish/dexmemfish_function.R")
source("/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plotScripts/dexmemfish/dexmemfish_function.R")
process_and_plot_data(inputdirectory, outputdirectory)
inputdirectory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/dexmemfish/rep7/all"
outputdirectory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/dexmemfish/rep7"
source("/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plotScripts/dexmemfish/dexmemfish_function.R")
process_and_plot_data(inputdirectory, outputdirectory)
inputdirectory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/dexmemfish/rep8/primary"
outputdirectory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/dexmemfish/rep8"
source("/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plotScripts/dexmemfish/dexmemfish_function.R")
process_and_plot_data(inputdirectory, outputdirectory)
inputdirectories <- c("/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/dexmemfish-altControls/dextimecoursestart/rep5",
"/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/dexmemfish-altControls/dextimecoursestart/rep6",
"/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/dexmemfish-altControls/dextimecoursestart/rep7"
)
outputdirectory <- "//Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/dexmemfish-altControls/dextimecoursestart"
# Function to safely convert to numeric
convert_to_numeric <- function(df, cols_to_convert) {
existing_cols <- intersect(cols_to_convert, names(df))
df[existing_cols] <- lapply(df[existing_cols], function(x) as.numeric(as.character(x)))
return(df)
}
# Columns to convert
cols_to_convert <- c("cy3", "a594", "cy5", "cy7","cell.Blob.Blob.metrics...Area","cell.Blob.Blob.metrics...Perimeter","cell.Blob.Blob.metrics...Centroid...y","cell.Blob.Blob.metrics...Centroid...x")
# Modified function to extract numeric identifier before the underscore
extract_first_number_after_dash <- function(filename) {
# Extracting the first number after the "-"
sub(".*-([0-9]+).*\\.csv$", "\\1", basename(filename))
}
# Initialize an empty list to store data from all directories
all_data_list <- list()
# Adding a unique dataset identifier for each input directory
dataset_id <- 1
# Iterate over each input directory and append data
for(inputdirectory in inputdirectories) {
files <- list.files(path = inputdirectory, pattern = "*.csv", full.names = TRUE)
data_list <- lapply(files, function(f) {
df <- read.csv(f, stringsAsFactors = FALSE)
df <- convert_to_numeric(df, cols_to_convert)
df$sample_identifier <- extract_first_number_after_dash(f) # Adding the modified identifier
df$dataset_identifier <- paste("Dataset", dataset_id) # Adding the dataset identifier
return(df)
})
all_data_list <- c(all_data_list, data_list) # Append data from current directory
dataset_id <- dataset_id + 1 # Increment dataset identifier for the next directory
}
# Combining all data into one dataframe
all_data <- bind_rows(all_data_list, .id = "file_id")
# Normalize cy3, a594, and cy5 by cy7
all_data <- all_data %>%
dplyr::mutate(cy3_norm = cy3 / cy7,
a594_norm = a594 / cy7,
cy5_norm = cy5 / cy7)
#normalize by area
all_data <- all_data %>%
dplyr::mutate(micron2 = `cell.Blob.Blob.metrics...Area` * 0.0467) %>%
dplyr::mutate(cy3_areanorm_RNApermicron2 = cy3 / `micron2`,
a594_areanorm_RNApermicron2 = a594 / `micron2`,
cy5_areanorm_RNApermicron2 = cy5 / `micron2`)
# Calculate the mean for each area-normalized variable, sample, and dataset
mean_areanorm_data <- all_data %>%
group_by(dataset_identifier, sample_identifier) %>%
dplyr::summarise(
cy3_mean = mean(cy3_areanorm_RNApermicron2, na.rm = TRUE),
cy3_se = sd(cy3_areanorm_RNApermicron2, na.rm = TRUE) / sqrt(n()),
a594_mean = mean(a594_areanorm_RNApermicron2, na.rm = TRUE),
a594_se = sd(a594_areanorm_RNApermicron2, na.rm = TRUE) / sqrt(n()),
cy5_mean = mean(cy5_areanorm_RNApermicron2, na.rm = TRUE),
cy5_se = sd(cy5_areanorm_RNApermicron2, na.rm = TRUE) / sqrt(n()),
.groups = "drop"
) %>%
pivot_longer(
cols = contains("_mean") | contains("_se"),
names_to = c(".value", "variable"),
names_sep = "_"
)
# Split the data into means and SEs
mean_data <- mean_areanorm_data %>%
filter(variable == "mean") %>%
dplyr::select(-variable)
se_data <- mean_areanorm_data %>%
filter(variable == "se") %>%
dplyr::select(-variable)
# Join means and SEs
analyte_data <- full_join(mean_data, se_data, by = c("dataset_identifier", "sample_identifier"), suffix = c("_mean", "_se"))
# Pivot longer to have one row per analyte per dataset_identifier per sample_identifier
long_data <- analyte_data %>%
pivot_longer(cols = -c(dataset_identifier, sample_identifier), names_pattern = "(.*)_(mean|se)", names_to = c("analyte", ".value"))
processed_data <- long_data
overall_means <- processed_data %>%
group_by(sample_identifier, analyte) %>%
summarise(overall_mean = mean(mean, na.rm = TRUE), .groups = 'drop')
dodge_width = .7
# Creating the plot with individual experiment bars and error bars
plot <- ggplot() +
# Add individual experiment bars with dodging
geom_bar(data = processed_data, aes(x = sample_identifier, y = mean, fill = dataset_identifier, group = interaction(sample_identifier, dataset_identifier, analyte)),
stat = "identity", position = position_dodge(width = dodge_width), width = 0.6) +
# Add error bars with dodging, aligned with individual bars
geom_errorbar(data = processed_data, aes(x = sample_identifier, ymin = mean - se, ymax = mean + se, group = interaction(sample_identifier, dataset_identifier, analyte)),
position = position_dodge(width = dodge_width), width = 0.25) +
# Overlay overall mean bars as translucent with a wider dodge to cover all individual bars
geom_bar(data = overall_means, aes(x = sample_identifier, y = overall_mean, fill = analyte, group = sample_identifier),
stat = "identity", position = position_dodge(width = dodge_width * 1.2), width = dodge_width * 1.1, alpha = 0.5, color = "black", show.legend = FALSE) +
facet_wrap(~analyte, scales = "free_y") +
theme_minimal() +
labs(title = "Mean Values with SE and Overall Mean for Each Condition", x = "Sample Identifier", y = "Mean Value") +
scale_fill_brewer(palette = "Set3")
print(plot)
ggsave(filename = paste0(outputdirectory, "/combineplottimecoursestart.pdf"), plot = plot, width = 12, height = 8, device = "pdf")
ggsave(filename = paste0(outputdirectory, "/combineplottimecoursestart.pdf"), plot = plot, width = 12, height = 8, device = "pdf")
input_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/barcodednaseq/20230726/exp5/output"
output_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/barcodednaseq/20230726"
shavedStarcode <- read.table(file.path(input_directory, "stepThreeStarcodeShavedReads.txt"), header = TRUE)
input_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/barcodednaseq/20230726/exp4/output"
output_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/barcodednaseq/20230726"
shavedStarcode <- read.table(file.path(input_directory, "stepThreeStarcodeShavedReads.txt"), header = TRUE)
#separate out each sample so that we can normalize to the number of reads total and set a cutoff that can be applied to all samples
#hi
a <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_13")
#lo
b <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_14")
#hi
c <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_15")
#lo->hi
d <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_17")
#lo+JNKi->hi
e <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_16")
sampletables <- c(a,c,b,d,e)
#subset 30bp barcodes   #collapse to combine UMIs
a <- a %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
b <- b %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
c <- c %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
d <- d %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
e <- e %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
#normalize to total UMIs
a$UMI = (a$UMI)/sum(a$UMI)*10000
b$UMI = (b$UMI)/sum(b$UMI)*10000
c$UMI = (c$UMI)/sum(c$UMI)*10000
d$UMI = (d$UMI)/sum(d$UMI)*10000
e$UMI = (e$UMI)/sum(e$UMI)*10000
#subset everything above count of 2 after normalization
a <- dplyr::filter(a, UMI > 2)
b <- dplyr::filter(b, UMI > 2)
c <- dplyr::filter(c, UMI > 2)
d <- dplyr::filter(d, UMI > 2)
e <- dplyr::filter(e, UMI > 2)
input_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/barcodednaseq/20230726/exp5/output"
output_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/barcodednaseq/20230726"
shavedStarcode <- read.table(file.path(input_directory, "stepThreeStarcodeShavedReads.txt"), header = TRUE)
#separate out each sample so that we can normalize to the number of reads total and set a cutoff that can be applied to all samples
#hi
a <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_18")
#lo
b <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_19")
#hi
c <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_20")
#lo->hi
d <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_21")
#lo+JNKi->hi
e <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_22")
sampletables <- c(a,c,b,d,e)
#subset 30bp barcodes   #collapse to combine UMIs
a <- a %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
b <- b %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
c <- c %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
d <- d %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
e <- e %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
#normalize to total UMIs
a$UMI = (a$UMI)/sum(a$UMI)*10000
b$UMI = (b$UMI)/sum(b$UMI)*10000
c$UMI = (c$UMI)/sum(c$UMI)*10000
d$UMI = (d$UMI)/sum(d$UMI)*10000
e$UMI = (e$UMI)/sum(e$UMI)*10000
#subset everything above count of 2 after normalization
a <- dplyr::filter(a, UMI > 2)
b <- dplyr::filter(b, UMI > 2)
c <- dplyr::filter(c, UMI > 2)
d <- dplyr::filter(d, UMI > 2)
e <- dplyr::filter(e, UMI > 2)
input_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/barcodednaseq/20230726/exp6/output"
output_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/barcodednaseq/20230726"
shavedStarcode <- read.table(file.path(input_directory, "stepThreeStarcodeShavedReads.txt"), header = TRUE)
#separate out each sample so that we can normalize to the number of reads total and set a cutoff that can be applied to all samples
#hi
a <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_23")
#lo
b <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_24")
#hi
c <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_25")
#lo->hi
d <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_26")
#lo+JNKi->hi
e <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_27")
sampletables <- c(a,c,b,d,e)
#subset 30bp barcodes   #collapse to combine UMIs
a <- a %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
b <- b %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
c <- c %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
d <- d %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
e <- e %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
#normalize to total UMIs
a$UMI = (a$UMI)/sum(a$UMI)*10000
b$UMI = (b$UMI)/sum(b$UMI)*10000
c$UMI = (c$UMI)/sum(c$UMI)*10000
d$UMI = (d$UMI)/sum(d$UMI)*10000
e$UMI = (e$UMI)/sum(e$UMI)*10000
#subset everything above count of 2 after normalization
a <- dplyr::filter(a, UMI > 2)
b <- dplyr::filter(b, UMI > 2)
c <- dplyr::filter(c, UMI > 2)
d <- dplyr::filter(d, UMI > 2)
e <- dplyr::filter(e, UMI > 2)
input_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/barcodednaseq/20230726/misc/exp6/output"
output_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/barcodednaseq/20230726"
shavedStarcode <- read.table(file.path(input_directory, "stepThreeStarcodeShavedReads.txt"), header = TRUE)
#separate out each sample so that we can normalize to the number of reads total and set a cutoff that can be applied to all samples
#hi
a <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_23")
#lo
b <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_24")
#hi
c <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_25")
#lo->hi
d <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_26")
#lo+JNKi->hi
e <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_27")
sampletables <- c(a,c,b,d,e)
#subset 30bp barcodes   #collapse to combine UMIs
a <- a %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
b <- b %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
c <- c %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
d <- d %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
e <- e %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
#normalize to total UMIs
a$UMI = (a$UMI)/sum(a$UMI)*10000
b$UMI = (b$UMI)/sum(b$UMI)*10000
c$UMI = (c$UMI)/sum(c$UMI)*10000
d$UMI = (d$UMI)/sum(d$UMI)*10000
e$UMI = (e$UMI)/sum(e$UMI)*10000
#subset everything above count of 2 after normalization
a <- dplyr::filter(a, UMI > 2)
b <- dplyr::filter(b, UMI > 2)
c <- dplyr::filter(c, UMI > 2)
d <- dplyr::filter(d, UMI > 2)
e <- dplyr::filter(e, UMI > 2)
overlap_results <- data.frame(Name = c("a_bc30", "b_bc30", "c_bc30", "d_bc30",
"a_c_overlap", "a_b_overlap", "b_c_overlap", "b_d_overlap", "a_d_overlap", "d_c_overlap"),
Length = sapply(list(a_bc30, b_bc30, c_bc30, d_bc30,
a_c_overlap, a_b_overlap, b_c_overlap, b_d_overlap, a_d_overlap, d_c_overlap), length))
input_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/extractedData/barcodednaseq/20230726/exp4/output"
output_directory <- "/Users/jessi/RajLab Dropbox/Jess Li/Shared_JessL/paper/plots/barcodednaseq/20230726/exp4"
shavedStarcode <- read.table(file.path(input_directory, "stepThreeStarcodeShavedReads.txt"), header = TRUE)
#separate out each sample so that we can normalize to the number of reads total and set a cutoff that can be applied to all samples
#hi
a <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_13")
#lo
b <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_14")
#hi
c <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_15")
#lo->hi
d <- dplyr::filter(shavedStarcode, SampleNum == "JL_AT_17")
sampletables <- c(a,c,b,d)
#subset 30bp barcodes   #collapse to combine UMIs
a <- a %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
b <- b %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
c <- c %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
d <- d %>% dplyr::select(UMI, BC30StarcodeD8, SampleNum) %>% group_by(BC30StarcodeD8) %>% summarise(UMI = sum(UMI))
#normalize to total UMIs
a$UMI = (a$UMI)/sum(a$UMI)*10000
b$UMI = (b$UMI)/sum(b$UMI)*10000
c$UMI = (c$UMI)/sum(c$UMI)*10000
d$UMI = (d$UMI)/sum(d$UMI)*10000
#subset everything above count of 2 after normalization
a <- dplyr::filter(a, UMI > 2)
b <- dplyr::filter(b, UMI > 2)
c <- dplyr::filter(c, UMI > 2)
d <- dplyr::filter(d, UMI > 2)
# Extracting the BC30StarcodeD8 columns
a_bc30 <- a$BC30StarcodeD8
b_bc30 <- b$BC30StarcodeD8
c_bc30 <- c$BC30StarcodeD8
d_bc30 <- d$BC30StarcodeD8
# Finding the common values
a_c_overlap <- intersect(a_bc30, c_bc30)
a_b_overlap <- intersect(a_bc30, b_bc30)
b_c_overlap <- intersect(b_bc30, c_bc30)
b_d_overlap <- intersect(b_bc30, d_bc30)
a_d_overlap <- intersect(a_bc30, d_bc30)
d_c_overlap <- intersect(d_bc30, c_bc30)
overlap_results <- data.frame(Name = c("a_bc30", "b_bc30", "c_bc30", "d_bc30",
"a_c_overlap", "a_b_overlap", "b_c_overlap", "b_d_overlap", "a_d_overlap", "d_c_overlap"),
Length = sapply(list(a_bc30, b_bc30, c_bc30, d_bc30,
a_c_overlap, a_b_overlap, b_c_overlap, b_d_overlap, a_d_overlap, d_c_overlap), length))
write.table(overlap_results, file = "overlap_results.txt", row.names = FALSE, col.names = TRUE, sep='\t')
library(VennDiagram)
# Create a list of sets
sets <- list(a = a_bc30, b = b_bc30, c = c_bc30, d = d_bc30)
# Function to create and save Venn diagram
create_venn <- function(set_list, set_names, output_file) {
venn <- venn.diagram(
x = set_list,
category.names = set_names,
filename = output_file,
output = TRUE,
imagetype = "png",
height = 3000,
width = 3000,
resolution = 300,
compression = "lzw",
lwd = 2,
col = "black",
alpha = 0.50,
label.col = "black",
cex = 1.5,
fontfamily = "sans",
fontface = "bold",
cat.cex = 1.5,
cat.fontfamily = "sans"
)
}
# Create all possible combinations of 2, 3, and 4 sets
combinations <- list(
list(c("a", "b"), "ab_venn.png"),
list(c("a", "c"), "ac_venn.png"),
list(c("a", "d"), "ad_venn.png"),
list(c("b", "c"), "bc_venn.png"),
list(c("b", "d"), "bd_venn.png"),
list(c("c", "d"), "cd_venn.png")
)
# Create and save Venn diagrams for all combinations
for (combo in combinations) {
set_names <- combo[[1]]
output_file <- file.path(output_directory, combo[[2]])
create_venn(sets[set_names], set_names, output_file)
}
